{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.10.14-cp310-cp310-win_amd64.whl.metadata (9.9 kB)\n",
      "Requirement already satisfied: absl-py in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (23.2.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (24.3.25)\n",
      "Requirement already satisfied: jax in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (0.4.31)\n",
      "Requirement already satisfied: jaxlib in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (0.4.31)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (3.6.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (1.25.2)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (4.6.0.66)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from mediapipe) (4.25.3)\n",
      "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
      "  Downloading sounddevice-0.5.1-py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.10 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jax->mediapipe) (1.13.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (4.37.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pycparser in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\proda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Downloading mediapipe-0.10.14-cp310-cp310-win_amd64.whl (50.8 MB)\n",
      "   ---------------------------------------- 50.8/50.8 MB 4.3 MB/s eta 0:00:00\n",
      "Downloading sounddevice-0.5.1-py3-none-win_amd64.whl (363 kB)\n",
      "Installing collected packages: sounddevice, mediapipe\n",
      "Successfully installed mediapipe-0.10.14 sounddevice-0.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi MediaPipe Hands untuk deteksi tangan\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "# Fungsi untuk memuat gambar dan label dari folder dengan MediaPipe\n",
    "def load_data_with_mediapipe(folder_path):\n",
    "    images = []\n",
    "    labels = []\n",
    "    classes = sorted(os.listdir(folder_path))  # Daftar huruf A-Z\n",
    "\n",
    "    for label, class_name in enumerate(classes):\n",
    "        class_path = os.path.join(folder_path, class_name)\n",
    "        \n",
    "        if not os.path.isdir(class_path):\n",
    "            continue  # Lewati file non-folder seperti desktop.ini jika ada\n",
    "        \n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                # Gunakan MediaPipe untuk mendeteksi tangan\n",
    "                rgb_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                result = hands.process(rgb_image)\n",
    "                \n",
    "                if result.multi_hand_landmarks:\n",
    "                    # Mendapatkan bounding box tangan berdasarkan landmark\n",
    "                    h, w, _ = img.shape\n",
    "                    landmarks = result.multi_hand_landmarks[0]\n",
    "                    x_min = int(min([lm.x for lm in landmarks.landmark]) * w)\n",
    "                    x_max = int(max([lm.x for lm in landmarks.landmark]) * w)\n",
    "                    y_min = int(min([lm.y for lm in landmarks.landmark]) * h)\n",
    "                    y_max = int(max([lm.y for lm in landmarks.landmark]) * h)\n",
    "\n",
    "                    # Crop ROI (Region of Interest) yang mengandung tangan\n",
    "                    roi = img[y_min:y_max, x_min:x_max]\n",
    "                    \n",
    "                    # Resize ROI ke ukuran yang diharapkan oleh model\n",
    "                    roi_resized = cv2.resize(roi, (300, 300))\n",
    "                    images.append(roi_resized)\n",
    "                    labels.append(label)  # Gunakan indeks folder sebagai label\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "# Muat dataset menggunakan fungsi dengan MediaPipe\n",
    "train_dir = \"DATASET/training\"\n",
    "val_dir = \"DATASET/validation\"\n",
    "test_dir = \"DATASET/test\"\n",
    "\n",
    "train_images, train_labels = load_data_with_mediapipe(train_dir)\n",
    "val_images, val_labels = load_data_with_mediapipe(val_dir)\n",
    "test_images, test_labels = load_data_with_mediapipe(test_dir)\n",
    "\n",
    "# Normalisasi gambar\n",
    "train_images = train_images / 255.0\n",
    "val_images = val_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images in training set: 494\n",
      "Image count per class in training set: {'A': 19, 'B': 19, 'C': 19, 'D': 19, 'E': 19, 'F': 19, 'G': 19, 'H': 19, 'I': 19, 'J': 19, 'K': 19, 'L': 19, 'M': 19, 'N': 19, 'O': 19, 'P': 19, 'Q': 19, 'R': 19, 'S': 19, 'T': 19, 'U': 19, 'V': 19, 'W': 19, 'X': 19, 'Y': 19, 'Z': 19}\n",
      "\n",
      "Total images in validation set: 78\n",
      "Image count per class in validation set: {'A': 3, 'B': 3, 'C': 3, 'D': 3, 'E': 3, 'F': 3, 'G': 3, 'H': 3, 'I': 3, 'J': 3, 'K': 3, 'L': 3, 'M': 3, 'N': 3, 'O': 3, 'P': 3, 'Q': 3, 'R': 3, 'S': 3, 'T': 3, 'U': 3, 'V': 3, 'W': 3, 'X': 3, 'Y': 3, 'Z': 3}\n",
      "\n",
      "Total images in test set: 26\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_images_in_folder(folder_path):\n",
    "    total_count = 0\n",
    "    class_counts = {}\n",
    "\n",
    "    # Cek apakah folder memiliki sub-folder (untuk kasus training dan validation)\n",
    "    if any(os.path.isdir(os.path.join(folder_path, d)) for d in os.listdir(folder_path)):\n",
    "        # Hitung jumlah gambar di setiap sub-folder (kelas)\n",
    "        for class_name in sorted(os.listdir(folder_path)):\n",
    "            class_path = os.path.join(folder_path, class_name)\n",
    "            \n",
    "            if not os.path.isdir(class_path):\n",
    "                continue  # Lewati file non-folder\n",
    "            \n",
    "            num_images = len(os.listdir(class_path))\n",
    "            class_counts[class_name] = num_images\n",
    "            total_count += num_images\n",
    "    else:\n",
    "        # Jika tidak ada sub-folder, hitung jumlah file langsung\n",
    "        total_count = len(os.listdir(folder_path))\n",
    "        class_counts = None  # Tidak ada kelas yang terdefinisi dalam folder test\n",
    "\n",
    "    return total_count, class_counts\n",
    "\n",
    "# Tentukan path dataset\n",
    "train_dir = \"DATASET/training\"\n",
    "val_dir = \"DATASET/validation\"\n",
    "test_dir = \"DATASET/test\"\n",
    "\n",
    "# Hitung jumlah data pada setiap set\n",
    "train_total, train_class_counts = count_images_in_folder(train_dir)\n",
    "val_total, val_class_counts = count_images_in_folder(val_dir)\n",
    "test_total, test_class_counts = count_images_in_folder(test_dir)\n",
    "\n",
    "print(\"Total images in training set:\", train_total)\n",
    "print(\"Image count per class in training set:\", train_class_counts)\n",
    "print(\"\\nTotal images in validation set:\", val_total)\n",
    "print(\"Image count per class in validation set:\", val_class_counts)\n",
    "print(\"\\nTotal images in test set:\", test_total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 468 images belonging to 26 classes.\n",
      "Found 52 images belonging to 26 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "# Mengatur direktori\n",
    "train_dir = \"DATASET/training\"\n",
    "val_dir = \"DATASET/validation\"\n",
    "test_dir = \"DATASET/test\"\n",
    "\n",
    "# Data Augmentation untuk training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "# Data untuk validasi dan testing\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "# Memuat data\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(300, 300),  # Sesuaikan dengan input_shape model Anda\n",
    "    batch_size=32,\n",
    "    class_mode='sparse',  # Gunakan 'sparse' untuk sparse categorical crossentropy\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(300, 300),\n",
    "    batch_size=32,\n",
    "    class_mode='sparse'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengatur input shape dan model dasar\n",
    "input_shape = (300, 300, 3)\n",
    "base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights=\"imagenet\")\n",
    "\n",
    "# Menambahkan layer baru\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dropout(0.5)(x)  # Menambahkan Dropout\n",
    "output = Dense(26, activation='softmax')(x)  # 26 kelas untuk huruf A-Z\n",
    "\n",
    "# Membangun model\n",
    "ssd_model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Freeze beberapa layer dari model dasar\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "ssd_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "15/15 [==============================] - 37s 2s/step - loss: 3.9407 - accuracy: 0.3675 - val_loss: 123.5190 - val_accuracy: 0.0577 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 1.6148 - accuracy: 0.6453 - val_loss: 132.0161 - val_accuracy: 0.0577 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 1.1275 - accuracy: 0.7244 - val_loss: 114.5155 - val_accuracy: 0.0962 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.9644 - accuracy: 0.8226 - val_loss: 76.4952 - val_accuracy: 0.2115 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.9222 - accuracy: 0.8419 - val_loss: 72.4320 - val_accuracy: 0.1538 - lr: 0.0010\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 33s 2s/step - loss: 1.1546 - accuracy: 0.8269 - val_loss: 63.0397 - val_accuracy: 0.3077 - lr: 0.0010\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.9991 - accuracy: 0.8590 - val_loss: 49.1791 - val_accuracy: 0.2692 - lr: 0.0010\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 1.2694 - accuracy: 0.8675 - val_loss: 43.1713 - val_accuracy: 0.4038 - lr: 0.0010\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.7657 - accuracy: 0.8803 - val_loss: 53.2696 - val_accuracy: 0.3269 - lr: 0.0010\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.7388 - accuracy: 0.9060 - val_loss: 73.3660 - val_accuracy: 0.3269 - lr: 0.0010\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.8119 - accuracy: 0.8889 - val_loss: 64.1577 - val_accuracy: 0.3654 - lr: 0.0010\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.7891 - accuracy: 0.9060 - val_loss: 62.8743 - val_accuracy: 0.3269 - lr: 0.0010\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.8763 - accuracy: 0.8996 - val_loss: 63.7993 - val_accuracy: 0.4038 - lr: 0.0010\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.4978 - accuracy: 0.9274 - val_loss: 55.3694 - val_accuracy: 0.3846 - lr: 2.0000e-04\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 33s 2s/step - loss: 0.1677 - accuracy: 0.9744 - val_loss: 50.5465 - val_accuracy: 0.3654 - lr: 2.0000e-04\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 34s 2s/step - loss: 0.2301 - accuracy: 0.9722 - val_loss: 50.3383 - val_accuracy: 0.3654 - lr: 2.0000e-04\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 33s 2s/step - loss: 0.1659 - accuracy: 0.9744 - val_loss: 52.2589 - val_accuracy: 0.3654 - lr: 2.0000e-04\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.0399 - accuracy: 0.9872 - val_loss: 51.1314 - val_accuracy: 0.3269 - lr: 2.0000e-04\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.0768 - accuracy: 0.9893 - val_loss: 48.3228 - val_accuracy: 0.3269 - lr: 4.0000e-05\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0552 - accuracy: 0.9915 - val_loss: 45.5879 - val_accuracy: 0.3269 - lr: 4.0000e-05\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0565 - accuracy: 0.9808 - val_loss: 42.1833 - val_accuracy: 0.3462 - lr: 4.0000e-05\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0442 - accuracy: 0.9893 - val_loss: 38.9619 - val_accuracy: 0.3654 - lr: 4.0000e-05\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0220 - accuracy: 0.9893 - val_loss: 35.9925 - val_accuracy: 0.3846 - lr: 4.0000e-05\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.0822 - accuracy: 0.9765 - val_loss: 33.1775 - val_accuracy: 0.4423 - lr: 4.0000e-05\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0345 - accuracy: 0.9915 - val_loss: 30.6142 - val_accuracy: 0.4423 - lr: 4.0000e-05\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0303 - accuracy: 0.9893 - val_loss: 28.5521 - val_accuracy: 0.4808 - lr: 4.0000e-05\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0659 - accuracy: 0.9893 - val_loss: 26.1236 - val_accuracy: 0.5000 - lr: 4.0000e-05\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0251 - accuracy: 0.9915 - val_loss: 23.9279 - val_accuracy: 0.5192 - lr: 4.0000e-05\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0242 - accuracy: 0.9915 - val_loss: 21.9760 - val_accuracy: 0.5192 - lr: 4.0000e-05\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0308 - accuracy: 0.9829 - val_loss: 19.9076 - val_accuracy: 0.5192 - lr: 4.0000e-05\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0323 - accuracy: 0.9957 - val_loss: 18.1358 - val_accuracy: 0.5192 - lr: 4.0000e-05\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.0554 - accuracy: 0.9850 - val_loss: 16.3741 - val_accuracy: 0.5192 - lr: 4.0000e-05\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0366 - accuracy: 0.9957 - val_loss: 14.7678 - val_accuracy: 0.5192 - lr: 4.0000e-05\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0110 - accuracy: 0.9957 - val_loss: 13.1512 - val_accuracy: 0.5192 - lr: 4.0000e-05\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0525 - accuracy: 0.9850 - val_loss: 11.5170 - val_accuracy: 0.5385 - lr: 4.0000e-05\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0181 - accuracy: 0.9936 - val_loss: 10.0680 - val_accuracy: 0.5962 - lr: 4.0000e-05\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0032 - accuracy: 0.9979 - val_loss: 8.8107 - val_accuracy: 0.5962 - lr: 4.0000e-05\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0451 - accuracy: 0.9936 - val_loss: 7.7602 - val_accuracy: 0.6346 - lr: 4.0000e-05\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0131 - accuracy: 0.9936 - val_loss: 6.7533 - val_accuracy: 0.6346 - lr: 4.0000e-05\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0386 - accuracy: 0.9957 - val_loss: 5.7398 - val_accuracy: 0.6538 - lr: 4.0000e-05\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.0206 - accuracy: 0.9957 - val_loss: 4.9247 - val_accuracy: 0.7115 - lr: 4.0000e-05\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0274 - accuracy: 0.9915 - val_loss: 4.1590 - val_accuracy: 0.7308 - lr: 4.0000e-05\n",
      "Epoch 43/50\n",
      "15/15 [==============================] - 32s 2s/step - loss: 0.0186 - accuracy: 0.9957 - val_loss: 3.4741 - val_accuracy: 0.7500 - lr: 4.0000e-05\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0187 - accuracy: 0.9936 - val_loss: 3.0325 - val_accuracy: 0.7692 - lr: 4.0000e-05\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0209 - accuracy: 0.9979 - val_loss: 2.4242 - val_accuracy: 0.7885 - lr: 4.0000e-05\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0028 - accuracy: 0.9979 - val_loss: 1.9394 - val_accuracy: 0.8269 - lr: 4.0000e-05\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0179 - accuracy: 0.9957 - val_loss: 1.5995 - val_accuracy: 0.8269 - lr: 4.0000e-05\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0044 - accuracy: 0.9979 - val_loss: 1.2931 - val_accuracy: 0.8654 - lr: 4.0000e-05\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0170 - accuracy: 0.9957 - val_loss: 1.0581 - val_accuracy: 0.8462 - lr: 4.0000e-05\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 31s 2s/step - loss: 0.0187 - accuracy: 0.9957 - val_loss: 0.8565 - val_accuracy: 0.9038 - lr: 4.0000e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2a65a08fcd0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model\n",
    "ssd_model.fit(train_generator,\n",
    "               validation_data=val_generator,\n",
    "               epochs=50,  # Sesuaikan jumlah epoch sesuai kebutuhan\n",
    "               callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluasi model\n",
    "test_loss, test_accuracy = ssd_model.evaluate(test_generator)\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}, Test loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model berhasil disimpan di MODEL/ssd_sibi_model_mediapipe.h5\n"
     ]
    }
   ],
   "source": [
    "# Simpan model\n",
    "model_path = \"MODEL/ssd_sibi_model_mediapipe.h5\"\n",
    "ssd_model.save(model_path)\n",
    "print(f\"Model berhasil disimpan di {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 764ms/step\n",
      "Predicted letter: L\n"
     ]
    }
   ],
   "source": [
    "# Fungsi untuk testing gambar dengan MediaPipe\n",
    "def test_with_mediapipe(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    rgb_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(rgb_image)\n",
    "    \n",
    "    if result.multi_hand_landmarks:\n",
    "        # Mendapatkan ROI dan resize\n",
    "        h, w, _ = img.shape\n",
    "        landmarks = result.multi_hand_landmarks[0]\n",
    "        x_min = int(min([lm.x for lm in landmarks.landmark]) * w)\n",
    "        x_max = int(max([lm.x for lm in landmarks.landmark]) * w)\n",
    "        y_min = int(min([lm.y for lm in landmarks.landmark]) * h)\n",
    "        y_max = int(max([lm.y for lm in landmarks.landmark]) * h)\n",
    "        \n",
    "        roi = img[y_min:y_max, x_min:x_max]\n",
    "        roi_resized = cv2.resize(roi, (300, 300)) / 255.0  # Normalisasi\n",
    "        \n",
    "        # Prediksi huruf dari gambar\n",
    "        roi_resized = np.expand_dims(roi_resized, axis=0)\n",
    "        prediction = ssd_model.predict(roi_resized)\n",
    "        predicted_class = np.argmax(prediction, axis=1)\n",
    "        return chr(predicted_class[0] + 65)  # Konversi ke huruf A-Z\n",
    "    else:\n",
    "        return \"No hand detected\"\n",
    "\n",
    "# Contoh penggunaan testing\n",
    "test_image_path = \"DATASET/test/W (3).jpg\"\n",
    "predicted_letter = test_with_mediapipe(test_image_path)\n",
    "print(f\"Predicted letter: {predicted_letter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)  # 0 untuk webcam pertama\n",
    "\n",
    "ssd_model = \"MODEL/ssd_sibi_model.h5\"\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Preprocessing frame\n",
    "    input_frame = cv2.resize(frame, (300, 300))\n",
    "    input_frame = np.expand_dims(input_frame, axis=0) / 255.0\n",
    "\n",
    "    # Prediksi label huruf SIBI\n",
    "    preds = ssd_model.predict(input_frame)\n",
    "    pred_label = np.argmax(preds, axis=1)[0]  # Dapatkan label prediksi\n",
    "    class_name = chr(pred_label + 65)  # Ubah indeks menjadi huruf A-Z\n",
    "\n",
    "    # Tampilkan hasil pada frame\n",
    "    cv2.putText(frame, f\"Predicted: {class_name}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "    cv2.imshow(\"Real-Time SIBI Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load model SSD yang telah dilatih\n",
    "model_path = \"MODEL/ssd_sibi_model.h5\"\n",
    "ssd_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Setup MediaPipe Hands\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Inisialisasi MediaPipe Hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
    "\n",
    "cap = cv2.VideoCapture(0)  # 0 untuk webcam pertama\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Ubah frame ke RGB\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Deteksi tangan dengan MediaPipe\n",
    "    results = hands.process(rgb_frame)\n",
    "\n",
    "    if results.multi_hand_landmarks:\n",
    "        for hand_landmarks in results.multi_hand_landmarks:\n",
    "            # Gambarkan landmark tangan\n",
    "            mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "            # Dapatkan bounding box dari posisi landmark\n",
    "            h, w, c = frame.shape\n",
    "            x_min = min([lm.x for lm in hand_landmarks.landmark]) * w\n",
    "            y_min = min([lm.y for lm in hand_landmarks.landmark]) * h\n",
    "            x_max = max([lm.x for lm in hand_landmarks.landmark]) * w\n",
    "            y_max = max([lm.y for lm in hand_landmarks.landmark]) * h\n",
    "\n",
    "            # Ekstrak ROI dari bounding box\n",
    "            roi = frame[int(y_min):int(y_max), int(x_min):int(x_max)]\n",
    "\n",
    "            # Resize ROI sesuai input model\n",
    "            if roi.size > 0:\n",
    "                roi_resized = cv2.resize(roi, (300, 300))\n",
    "                roi_normalized = np.expand_dims(roi_resized / 255.0, axis=0)\n",
    "\n",
    "                # Prediksi huruf dengan model SSD\n",
    "                preds = ssd_model.predict(roi_normalized)\n",
    "                pred_label = np.argmax(preds, axis=1)[0]\n",
    "                class_name = chr(pred_label + 65)  # Ubah indeks ke huruf A-Z\n",
    "\n",
    "                # Tampilkan prediksi di frame\n",
    "                cv2.putText(frame, f\"Predicted: {class_name}\", (int(x_min), int(y_min) - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)\n",
    "\n",
    "    # Tampilkan frame\n",
    "    cv2.imshow(\"Real-Time SIBI Detection with MediaPipe\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "hands.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
